{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGnVrI09wA5G"
      },
      "source": [
        "# Clustering Pruebas Images \n",
        "\n",
        "In this notebook, we will analyse the [\"Pruebas\" plant images](https://drive.google.com/file/d/1g2gHE5vp7-FfWnFTiO29vMLemUFyvCSM/view?usp=sharing). This data set contians 378 usable images and the zipped folder has been stored in the above link. All the images are 2832X 4240 color images.\n",
        "Author: pc2846@g.rit.edu\n",
        "\n",
        "\n",
        "## Cleaning the Data:\n",
        "\n",
        "The folder contained some random images that had to be removed for this project. The images are captured in a time lapse, therefore we decided to group the images manually into 9 folders. We assumed that this manual process will help us clean the data and have a base for comparing the model's performance with the human's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Essential Libraries\n",
        "\n",
        "ScikitLearn and TensorFlow are the major APIs being used for modeling."
      ],
      "metadata": {
        "id": "OmrAOCrEJt79"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k17ax06nsQLU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset \n",
        "Downloading the zipped folder of dataset into the google colab from the drive. Later unzipping the folder to access the folders inside the main directory. We see 9 folders created after cleaning the dataset. "
      ],
      "metadata": {
        "id": "yM3Z9RPVKktx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive to locate the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JXtjeLiBGhEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b66356-9933-4f37-b3ea-0727bdc68b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctVZaXrTugeh"
      },
      "outputs": [],
      "source": [
        "# Unzip the archive\n",
        "import zipfile\n",
        "local_zip = '/content/drive/MyDrive/Quick Essential Docs./Quick_Projects/projects/Luciferase/train.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_cmmE1t9aRy"
      },
      "source": [
        "## Sanity Check Before Analysis\n",
        "It is important to understand and check if all the images are accessible, we find 9 folders and some images stored in all of them. We count the number of images present in each of the folders so that we have a total number of files to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfRF8Z5P5HvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91632abd-9183-4e89-d899-7f4732a6a2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training images in train/c1: 3\n",
            "total training images in train/c2: 3\n",
            "total training images in train/c3: 26\n",
            "total training images in train/c4: 47\n",
            "total training images in train/c5: 63\n",
            "total training images in train/c6: 90\n",
            "total training images in train/c7: 41\n",
            "total training images in train/c8: 71\n",
            "total training images in train/c9: 34\n",
            "\n",
            "total training images: 378\n",
            "\n",
            "File names in every folder: \n",
            "train/c1: ['DSC00001(1).JPG', 'DSC00005.JPG', 'DSC00004.JPG']\n",
            "train/c2: ['DSC00001.JPG', 'DSC00002.JPG', 'DSC00003.JPG']\n",
            "train/c3: ['DSC00022.JPG', 'DSC00021.JPG', 'DSC00014.JPG', 'DSC00013.JPG', 'DSC00017.JPG', 'DSC00029.JPG', 'DSC00011.JPG', 'DSC00020.JPG', 'DSC00030.JPG', 'DSC00023.JPG']\n",
            "train/c4: ['DSC00032.JPG', 'DSC00063.JPG', 'DSC00068.JPG', 'DSC00050.JPG', 'DSC00058.JPG', 'DSC00060.JPG', 'DSC00045.JPG', 'DSC00062.JPG', 'DSC00061.JPG', 'DSC00059.JPG']\n",
            "train/c5: ['DSC00107.JPG', 'DSC00114.JPG', 'DSC00110.JPG', 'DSC00098.JPG', 'DSC00115.JPG', 'DSC00125.JPG', 'DSC00140.JPG', 'DSC00104.JPG', 'DSC00116.JPG', 'DSC00119.JPG']\n",
            "train/c6: ['DSC00221.JPG', 'DSC00170.JPG', 'DSC00161.JPG', 'DSC00201.JPG', 'DSC00206.JPG', 'DSC00203.JPG', 'DSC00155.JPG', 'DSC00168.JPG', 'DSC00222.JPG', 'DSC00197.JPG']\n",
            "train/c7: ['DSC00239.JPG', 'DSC00260(1).JPG', 'DSC00265(2).JPG', 'DSC00258(1).JPG', 'DSC00267(2).JPG', 'DSC00258(1)(1).JPG', 'DSC00252.JPG', 'DSC00233.JPG', 'DSC00250.JPG', 'DSC00267(2)(1).JPG']\n",
            "train/c8: ['DSC00333.JPG', 'DSC00272(2).JPG', 'DSC00325.JPG', 'DSC00311.JPG', 'DSC00334.JPG', 'DSC00275(2).JPG', 'DSC00330.JPG', 'DSC00313.JPG', 'DSC00327.JPG', 'DSC00290.JPG']\n",
            "train/c9: ['DSC00343.JPG', 'DSC00367.JPG', 'DSC00357.JPG', 'DSC00345.JPG', 'DSC00346.JPG', 'DSC00368.JPG', 'DSC00356.JPG', 'DSC00366.JPG', 'DSC00364.JPG', 'DSC00369.JPG']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_dir = 'train'\n",
        "lst_folders = os.listdir(base_dir)\n",
        "\n",
        "# Sanity check\n",
        "base_dir = 'train'\n",
        "c1_dir = os.path.join(base_dir, 'c1')\n",
        "c2_dir = os.path.join(base_dir, 'c2')\n",
        "c3_dir = os.path.join(base_dir, 'c3')\n",
        "c4_dir = os.path.join(base_dir, 'c4')\n",
        "c5_dir = os.path.join(base_dir, 'c5')\n",
        "c6_dir = os.path.join(base_dir, 'c6')\n",
        "c7_dir = os.path.join(base_dir, 'c7')\n",
        "c8_dir = os.path.join(base_dir, 'c8')\n",
        "c9_dir = os.path.join(base_dir, 'c9')\n",
        "\n",
        "# List of folders inside the train directory\n",
        "lst_of_dirs = [c1_dir, c2_dir, c3_dir, c4_dir, c5_dir, c6_dir, c7_dir, c8_dir, c9_dir]\n",
        "\n",
        "# Printing the number of files in each folder an the total number of images\n",
        "sum_of_imgs = 0\n",
        "for dir in lst_of_dirs:\n",
        "  sum_of_imgs += len(os.listdir(dir))\n",
        "  print('total training images in ' + dir + ':' , len(os.listdir(dir)))\n",
        "\n",
        "print('\\ntotal training images: ' + str(sum_of_imgs))\n",
        "print()\n",
        "\n",
        "# Printing few file names under each folder\n",
        "print(\"File names in every folder: \")\n",
        "for dir in lst_of_dirs:\n",
        "  print(dir + \": \" + str(os.listdir(dir)[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zE0a0SEP_rN"
      },
      "source": [
        "## Analyzing the Images\n",
        "Two images from each folder are displayed. We understand that a pattern exists where the brightness of the plants (bioluminescence) increases till a point and then starts to fall when the images are displayed in the captured order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60qJRqUn8yfM"
      },
      "outputs": [],
      "source": [
        "# Expectation: Since the images are time series, images from each foler must\n",
        "# show us developement of plants in wells\n",
        "\n",
        "# Utilities for displaying image to check the logic \n",
        "folder_file_dict = {}\n",
        "for dir in lst_of_dirs:\n",
        "  if dir not in folder_file_dict:\n",
        "    name = dir[-2:] + '_files'\n",
        "    folder_file_dict[name] = []\n",
        "\n",
        "for k in folder_file_dict.keys():\n",
        "  for dir in lst_of_dirs:\n",
        "    if k[:2] == dir[-2:]:\n",
        "      folder_file_dict[k].extend(os.listdir(dir))\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Sorted list of images\n",
        "lst_of_cx_files = []\n",
        "for v in folder_file_dict.values():\n",
        "  lst_of_cx_files.append(sorted(v))\n",
        "\n",
        "# List of 2 images \n",
        "pic_index = 2\n",
        "display_lst = []\n",
        "for cx_dir, cx_files in zip(lst_of_dirs, lst_of_cx_files):\n",
        "  d = [os.path.join(cx_dir, fname) for fname in sorted(cx_files[pic_index-2:pic_index])]\n",
        "  display_lst.append(d)\n",
        "\n",
        "# List of images to be displayed\n",
        "final_display = []\n",
        "for d in display_lst:\n",
        "  final_display += d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowing the Size of Image\n",
        "This step can help us understand our dataset better. We have to preprocess all the images into a size that is acceptable by the model that we will be using for feature extraction."
      ],
      "metadata": {
        "id": "6-O_iDQROyDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SucAEpOZtjhQ"
      },
      "outputs": [],
      "source": [
        "# Choosing a random image\n",
        "import secrets\n",
        "rand_sample_path = secrets.choice(final_display)\n",
        "sample_image  = load_img(rand_sample_path)\n",
        "# Convert the image into its numpy array representation\n",
        "sample_array = img_to_array(sample_image)\n",
        "print(f\"Random image has a shape of: {sample_array.shape}\")\n",
        "\n",
        "# Check if all the images are of the same shape\n",
        "all_paths = []\n",
        "for cx_dir, cx_files in zip(lst_of_dirs, lst_of_cx_files):\n",
        "  l = [os.path.join(cx_dir, fname) for fname in cx_files[:]]\n",
        "  all_paths.append(l)\n",
        "\n",
        "flat_lst_paths = [item for l in all_paths for item in l]\n",
        "rnd_img_shape = sample_array.shape\n",
        "cnt = 0\n",
        "tot_imgs = len(flat_lst_paths)\n",
        "for path in flat_lst_paths:\n",
        "  _image  = load_img(path)\n",
        "  _array = img_to_array(_image)\n",
        "  if rnd_img_shape == _array.shape:\n",
        "    cnt += 1\n",
        "if cnt == tot_imgs:\n",
        "  print(\"\\nAll images are of the same size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8FX1wxSoui_"
      },
      "source": [
        "## Data Preprocessing and Feature Extraction\n",
        "The images in the raw form are of size 2832X4240 which is not acceptable by the VGG16 model. Therefore we resize the input images into 224X224 NumPy arrays. \n",
        "\n",
        "We will be using the VGG16 model as a feature extractor, this model takes in batches of images rather than a single one. Therefore we reshape the image as (1, 224, 224, 3). Below function takes one of our image and object of our instantiated model to preprocess the image and to return the features. The output layer of the VGG model is removed so that the new final layer is a fully-connected layer with 4,096 output nodes. This vector of 4,096 numbers is the feature vector that we will use to cluster the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqb4iEhjroCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9785cbfb-a36a-4f12-8469-056ad7dc0b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        }
      ],
      "source": [
        "# Importing the model and methods for transfer learning\n",
        "from keras.applications.vgg16 import VGG16 \n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import preprocess_input \n",
        "\n",
        "# load the model first and pass as an argument\n",
        "model = VGG16()\n",
        "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
        "\n",
        "def extract_features(file, model):\n",
        "    # load the image as a 224x224 array\n",
        "    img = load_img(file, target_size=(224,224))\n",
        "    # convert from image to numpy array\n",
        "    img = img_to_array(img)\n",
        "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
        "    reshaped_img = img.reshape(1,224,224,3) \n",
        "    # prepare image for model\n",
        "    imgx = preprocess_input(reshaped_img)\n",
        "    # get the feature vector\n",
        "    features = model.predict(imgx, use_multiprocessing=True)\n",
        "    return features\n",
        "\n",
        "# The extracted features are stored in the HashMap otherwise stored in the \n",
        "# pickle file\n",
        "all_images = flat_lst_paths\n",
        "data = {}\n",
        "p = r\"/content/plant_features.pkl\"\n",
        "\n",
        "# loop through each image in the dataset\n",
        "for image in all_images:\n",
        "    # try to extract the features and update the dictionary\n",
        "    try:\n",
        "        feat = extract_features(image, model)\n",
        "        data[image] = feat\n",
        "    # if something fails, save the extracted features as a pickle file (optional)\n",
        "    except:\n",
        "        with open(p,'wb') as file:\n",
        "            pickle.dump(data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjBzvlbDraL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63f1bba-db8f-4acc-d5eb-0f967fa121ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(378, 1, 4096)\n",
            "(378, 4096)\n"
          ]
        }
      ],
      "source": [
        "# List of the filenames\n",
        "filenames = np.array(list(data.keys()))\n",
        "# List of just the features\n",
        "feat = np.array(list(data.values()))\n",
        "# Reshaping the features\n",
        "print(feat.shape)\n",
        "feat = feat.reshape(-1,4096)\n",
        "print(feat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction with PCA\n",
        "Inorder to address the Curse of Dimensionality we use PCA to reduce the number of features from 4096 to a smaller number. We statistically analyse the features to pick a small number for the dimensionality reduction. Typically, we want the explained variance to be between 95–99% from the below graph we statistically find out that **50** or **25** components will be the best for reduction."
      ],
      "metadata": {
        "id": "PmGI5e19vZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=50, svd_solver='full', random_state=22)\n",
        "pca.fit(feat)\n",
        "x = pca.transform(feat)\n",
        "print(\"Features of \" + str(feat.shape[0]) + \" images before dimensionality reduction with PCA: \" + str(feat.shape[1]))\n",
        "print(\"Features of \" + str(x.shape[0]) + \" images before dimensionality reduction with PCA: \" + str(x.shape[1]))"
      ],
      "metadata": {
        "id": "qXTNz9hTjtHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c1716d-7385-48c8-efe2-2bd507c799e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features of 378 images before dimensionality reduction with PCA: 4096\n",
            "Features of 378 images before dimensionality reduction with PCA: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans Clustering\n",
        "## Checking the number of clusters with Elbow method.\n",
        "### Looks like 3 clusters is the best fit for this scenario."
      ],
      "metadata": {
        "id": "ccweJQ5RjjTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans training on 302 images"
      ],
      "metadata": {
        "id": "4QZFNUZM-lKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x[:302]\n",
        "x_validation = x[302:]"
      ],
      "metadata": {
        "id": "-ecu158golj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=22)\n",
        "kmeans.fit(x_train)\n",
        "labels = kmeans.labels_\n",
        "print(str(len(np.unique(labels))) + \" labels generated for \" + str(len(labels)) + \" images\")\n",
        "print(\"Unique ID's in generated labels: \" + str(np.unique(labels)))"
      ],
      "metadata": {
        "id": "-F_rA1L781Iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc6539c-0cc6-46cf-ef4f-9346cd1ec331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 labels generated for 302 images\n",
            "Unique ID's in generated labels: [0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save K-Means model for CL and PCA as pickle file"
      ],
      "metadata": {
        "id": "OSEcV9QuCjj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(kmeans, open('kmeans_luciferase_cl.pkl', 'wb'))\n",
        "pickle.dump(pca, open('pca_luciferase.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "4nFxXjsb0jk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}