{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_UccvLRvLtW"
      },
      "source": [
        "## Streamlit App with Kmeans and Continual Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfH7W_vjjHzu",
        "outputId": "12859bea-0cbc-4a5a-f048-88d9fc0fc6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.21.0-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.1.3)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (13.3.4)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.22.4)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2.2)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.6.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.9/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: pandas<2,>=0.25 in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.27.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.3)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (5.3.0)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit) (3.15.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2,>=0.25->streamlit) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->streamlit) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2.0.12)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit) (2.14.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair<5,>=3.2.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit) (2023.3)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=df69d87c03d71e41378edcae31e18db94ab88df03eab31c104efd22b1f56a921\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, smmap, pympler, blinker, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed blinker-1.6.2 gitdb-4.0.10 gitpython-3.1.31 pydeck-0.8.1b0 pympler-1.0.1 smmap-5.0.0 streamlit-1.21.0 validators-0.20.0 watchdog-3.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19879 sha256=152b5f1a9069d5bbbe1d5da407872d3e40ca7aa4b3d15645317dfe1c9a26ef31\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/49/9c/44b13823eb256a3b4dff34b972f7a3c7d9910bfef269e59bd7\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81-sqsVNNt7A"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "# create a new storage for unseen images\n",
        "with open('storage_new_imgs.csv', mode='w') as file:\n",
        "    writer = csv.writer(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPL6IqTSjJJm",
        "outputId": "be42b373-575d-4632-fbc6-d246799a1885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import csv\n",
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Importing the model and methods for transfer learning\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "def extract_features(target_file):\n",
        "    \"\"\"\n",
        "    This function preprocesses the input target image\n",
        "    and loades the VGG 16 model to extract features.\n",
        "    return: Extracted features with dimenions 4096\n",
        "    \"\"\"\n",
        "    # DL model for transfer learning\n",
        "    model = VGG16()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    # load the image as a 224x224 array\n",
        "    img = load_img(target_file, target_size=(224, 224))\n",
        "    # convert from image to numpy array\n",
        "    img = img_to_array(img)\n",
        "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
        "    reshaped_img = img.reshape(1, 224, 224, 3)\n",
        "    # prepare image for model\n",
        "    imgx = preprocess_input(reshaped_img)\n",
        "    # get the feature vector\n",
        "    features = model.predict(imgx, use_multiprocessing=True)\n",
        "    return features\n",
        "\n",
        "def preprocess_input_img_for_kmeans(input_target_img_path):\n",
        "    \"\"\"\n",
        "    This function uses the extract features function to preprocess\n",
        "    and extract the features from the target input image and reduces\n",
        "    the high dimensional features (4096) to 50.\n",
        "    return: Reduced features with dimensions 50\n",
        "    \"\"\"\n",
        "    # Extracting the features from the target input image\n",
        "    img_feat = extract_features(input_target_img_path)\n",
        "    # Loading PCA pkl file\n",
        "    pca_pkl = \"pca_luciferase.pkl\"\n",
        "    pca = pickle.load(open(pca_pkl, \"rb\"))\n",
        "    # Reducing high dimensionality\n",
        "    reduced_feat = pca.transform(img_feat)\n",
        "    return reduced_feat\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    This function open a pkl file and and loads the ML model\n",
        "    return: ML model\n",
        "    \"\"\"\n",
        "    with open('kmeans_luciferase_cl.pkl', 'rb') as f:\n",
        "      model = pickle.load(f)\n",
        "      return model\n",
        "\n",
        "def save_new_model(model):\n",
        "    \"\"\"\n",
        "    This function open the pkl file to write updated ML model\n",
        "    return: None\n",
        "    \"\"\"\n",
        "    with open('kmeans_luciferase_cl.pkl', 'wb') as f:\n",
        "      pickle.dump(model, f)\n",
        "\n",
        "# Stremlit app's code starts below:\n",
        "st.title('Unsupervised Clustering of Luciferase Images')\n",
        "st.markdown(\"Streamlit web application to identify cluster ID of a given plant image 🌱🌿🍃 using K-means\")\n",
        "uploaded_file = st.file_uploader('upload image', type = 'jpg')\n",
        "if uploaded_file is not None:\n",
        "    # Displays uploaded image\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "    # Preprocess input image for K-Means\n",
        "    st.markdown('Preprocessing uploaded image...')\n",
        "    reduced_feat = preprocess_input_img_for_kmeans(uploaded_file)\n",
        "    # Open CSV file for appending\n",
        "    with open('storage_new_imgs.csv', 'a', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow(reduced_feat[0])\n",
        "    model = load_model()\n",
        "    # Checks number of unseen images stored and takes required action\n",
        "    with open('storage_new_imgs.csv', 'r') as csvfile:\n",
        "      csvreader = csv.reader(csvfile)\n",
        "      new_img_feat_count = sum(1 for row in csvreader)\n",
        "    if new_img_feat_count < 3:\n",
        "      output_prediction_arr = model.predict(reduced_feat)\n",
        "      cnt_of_predictions = 2\n",
        "      distance_from_centroids = model.transform(reduced_feat)\n",
        "      output_prediction = output_prediction_arr[0] \n",
        "      loc_centroid_1 = model.cluster_centers_[0]\n",
        "      loc_centroid_2 = model.cluster_centers_[1]\n",
        "      loc_centroid_3 = model.cluster_centers_[2]\n",
        "      success_msg = f'''\n",
        "                    Prediction generated from original model!\n",
        "\n",
        "                    Current number of new images collected: {new_img_feat_count}\n",
        "\n",
        "                    Distance from centroids: {distance_from_centroids}\n",
        "\n",
        "                    Uploaded image belongs to cluster: {output_prediction}/{cnt_of_predictions}\n",
        "                    '''\n",
        "      st.success(success_msg)\n",
        "    \n",
        "    # model update happens if number of unseen images reaches 3\n",
        "    else:\n",
        "      existing_file_path = \"/content/kmeans_luciferase.pkl\"\n",
        "      prev_weights = model.cluster_centers_\n",
        "      st.markdown('Intiating continual learning...')\n",
        "      time.sleep(0.01)\n",
        "      st.markdown('Learning from new set of images...')\n",
        "      # reading all 3 unseen images from csv file\n",
        "      with open('storage_new_imgs.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        new_img_feat_lst = []\n",
        "        for row in csv_reader:\n",
        "          float_row = [float(value) for value in row]\n",
        "          new_img_feat_lst.append(float_row)\n",
        "      # updating model on 3 new images\n",
        "      model.fit(new_img_feat_lst[:])\n",
        "      new_weights = model.cluster_centers_\n",
        "      weight_decay = 0.5\n",
        "      penalized_weights = (1 - weight_decay) * new_weights + weight_decay * prev_weights\n",
        "      model.cluster_centers_ = penalized_weights\n",
        "      \n",
        "      # overwrite the kmeans_luciferase_cl.pkl file with the updated model\n",
        "      # Check for existing model and delete it\n",
        "      if os.path.exists(existing_file_path):\n",
        "        os.remove(existing_file_path)\n",
        "\n",
        "      # saving new version of model\n",
        "      save_new_model(model)\n",
        "\n",
        "      # loading new version of model\n",
        "      model = load_model()\n",
        "      \n",
        "      # clears list and csv of new image features\n",
        "      new_img_feat_lst.clear()\n",
        "      with open('storage_new_imgs.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([]) \n",
        "\n",
        "      # outputs prediction result\n",
        "      output_prediction_arr = model.predict(reduced_feat.astype(np.float64))\n",
        "      cnt_of_predictions = 2\n",
        "      output_prediction = output_prediction_arr[0]\n",
        "      distance_from_centroids = model.transform(reduced_feat.astype(np.float64))\n",
        "      new_loc_centroid_1 = model.cluster_centers_[0]\n",
        "      new_loc_centroid_2 = model.cluster_centers_[1]\n",
        "      new_loc_centroid_3 = model.cluster_centers_[2]\n",
        "\n",
        "      success_msg = f'''\n",
        "                    Prediction generated from updated model trained on unseen set of 3 images\n",
        "\n",
        "                    Distance from centroids: {distance_from_centroids}\n",
        "\n",
        "                    Uploaded image belongs to cluster: {output_prediction}/{cnt_of_predictions}\n",
        "                    '''\n",
        "      st.success(success_msg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqplo8A1q6k2"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py&>/dev/null&"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF27WvdHjQm8",
        "outputId": "29a45e65-3771-4e9b-ede8-3de8c12e69f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NRXnARWJZFxh7ReAePMyKr33CQ_7gdZTBPyz4SWTpBpY1itv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEXTzkUHlcJe",
        "outputId": "c1dbb89e-a0a9-4044-efd4-f17ca6faf39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-26 18:09:33--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  14.7MB/s    in 0.9s    \n",
            "\n",
            "2023-04-26 18:09:35 (14.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xijm4NCTlgyo",
        "outputId": "a2940e6b-f693-488d-80ac-f6896d766537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ],
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmrvymr6l5v4"
      },
      "outputs": [],
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VayraTXmmTfQ",
        "outputId": "8d68dd13-19ff-473b-e547-edb941a1d3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://968d-34-82-216-147.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUXBAdLnnHaz",
        "outputId": "ec55af7b-a380-4f85-dc0c-9385624ee2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8502\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.82.216.147:8502\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKi-jjbrs00Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}